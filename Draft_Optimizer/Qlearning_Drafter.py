"""
This file constructs and trains a multi-agent Q-Learning algorithm with epsilon-greedy exploration to optimally draft
a fantasy football team.
We take as input the Best_Ball_Draft_Board.cvs generated by Best_Ball_Draft_Board.py
This is very much a work in progress with this being the first practice step in implementing increasingly complex
algorithms.
"""

import pandas as pd
import random
import matplotlib.pyplot as plt
from collections import defaultdict


class QAgent:
    def __init__(self, team_id, learning_rate=0.25, discount_factor=0.9, epsilon=1.0, epsilon_decay=0.995,
                 epsilon_min=0.01):
        """ Initialize an individual agent for a team. """
        self.team_id = team_id  # Team identification for this agent
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.q_table = defaultdict(float)  # Q-table to store state-action values for this agent
        self.drafted_players = []  # List to store drafted players for this agent
        self.total_reward = 0  # Store the total accumulated reward for this agent
        self.total_points = 0  # Store total projected fantasy points for this agent.
        self.position_counts = {position: 0 for position in position_limits}  # Track drafted positions

    def reset_agent(self):
        """Reset the agent's state for a new episode."""
        self.drafted_players = []
        self.total_reward = 0
        self.total_points = 0
        self.position_counts = {position: 0 for position in position_limits}

    def get_state(self, round_number):
        """Get the current state representation for the agent."""
        return (tuple(sorted(self.position_counts)), round_number)

    def choose_action(self, state, available_players):
        """Choose an action using an epsilon-greedy policy."""
        if random.random() < self.epsilon:  # With probability epsilon, choose a random action.
            return random.choice(available_players.index.tolist())
        else:  # Otherwise, choose the best action.
            return max(available_players.index, key=lambda player: self.q_table[(state, player)])

    def update_q_table(self, state, action, reward, next_state, available_players):
        """Update the Q-table using the Q-learning formula."""
        best_next_action = max(available_players.index, key=lambda player: self.q_table[(next_state, player)],
                               default=0)
        td_target = reward + self.discount_factor * self.q_table[(next_state, best_next_action)]
        td_delta = td_target - self.q_table[(state, action)]
        self.q_table[(state, action)] += self.learning_rate * td_delta


class FantasyDraft:
    def __init__(self, player_data, num_teams, num_rounds):
        """ Initialize the multi-agent draft simulation. """
        self.player_data = player_data  # Expects a pandas DataFrame.
        self.num_teams = num_teams
        self.num_rounds = num_rounds
        self.agents = [QAgent(team_id=i) for i in range(num_teams)]  # Initialize an agent for each team.
        self.reset_draft()
        self.reward_history = {i: [] for i in range(num_teams)}  # Track rewards for debug purposes.
        self.epsilon_history = {i: [] for i in range(self.num_teams)}  # Track epsilon for each agent
        self.draft_order = list(range(num_teams))

    def reset_draft(self):
        """Reset the draft for a new episode."""
        self.available_players = self.player_data.copy()
        self.current_round = 0
        self.current_team = 0
        self.draft_order = list(range(self.num_teams))  # Reset draft order
        for agent in self.agents:
            agent.reset_agent()

    def run_episode(self, verbose=False):
        """Run a single episode of the draft."""
        self.reset_draft()
        while self.current_round < self.num_rounds:
            for team in self.draft_order:
                agent = self.agents[team]
                state = agent.get_state(self.current_round)

                # Filter available players to respect position limits
                valid_players = self.available_players[
                    self.available_players['position'].apply(
                        lambda pos: agent.position_counts[pos] < position_limits[pos]
                    )
                ]

                # Check if there are any draftable players.
                if valid_players.empty:
                    raise Exception("There are no valid players for the agent to draft from!")

                action = agent.choose_action(state, valid_players)

                drafted_player = self.available_players.loc[action]
                agent.total_points += drafted_player["projected_points"]
                agent.drafted_players.append(drafted_player["player_name"])
                agent.position_counts[drafted_player["position"]] += 1  # Increment position count

                # Compute reward for this action and normalize.
                reward = self.get_reward(drafted_player) / player_data["projected_points"].max()
                agent.total_reward += reward

                self.available_players = self.available_players.drop(action)

                next_state = agent.get_state(self.current_round + 1)
                agent.update_q_table(state, action, reward, next_state, self.available_players)

            self.current_round += 1  # Move to next round after all teams have picked.
            self.draft_order.reverse()  # Reverse the draft order for snake draft formats.

        if verbose:
            for agent in self.agents:
                print(
                    f"  Team {agent.team_id}: Total Reward = {round(agent.total_reward, 2)}, Drafted Players = {agent.drafted_players} ({round(agent.total_points, 2)} pts)")

    def get_reward(self, drafted_player):
        """Calculate the reward attained for drafting a given player"""
        proj_points = drafted_player["projected_points"]

        # Get total value lost from not picking the best possible player.
        max_points_by_position = self.available_players.groupby("position")["projected_points"].max()
        loss_penalty = drafted_player["projected_points"] - max_points_by_position[drafted_player["position"]]

        total_reward = proj_points + loss_penalty
        return total_reward

    def train(self, num_episodes, verbose=False):
        """Train the agents over multiple episodes."""
        for episode in range(num_episodes):
            self.run_episode(verbose=False)
            for agent in self.agents:
                agent.epsilon = max(agent.epsilon * agent.epsilon_decay, agent.epsilon_min)  # decay epsilon value.
                self.epsilon_history[agent.team_id].append(agent.epsilon)  # Log epsilon values
                self.reward_history[agent.team_id].append(agent.total_reward)  # Log rewards for debug purposes.
            # Print episode summary
            if verbose:
                print(f"Episode {episode + 1}/{num_episodes} completed.")
                for agent in self.agents:
                    print(
                        f"  Team {agent.team_id}: Total Reward = {round(agent.total_reward, 2)}, Drafted Players = {agent.drafted_players} ({round(agent.total_points, 2)} pts)")

    def plot_rewards(self):
        """Plot the learning progress for debug purposes."""
        # Plot total rewards
        plt.figure(figsize=(12, 6))
        for team_id, rewards in self.reward_history.items():
            # Compute a moving average for total rewards.
            smoothed_rewards = pd.Series(rewards).rolling(window=50).mean()
            plt.plot(smoothed_rewards, label=f"Team {team_id + 1} Total Rewards")

        # Overlay vertical lines to represent the start of each phase
        phase_starts = [sum(num_episodes[:i]) for i in range(1, len(num_episodes))]
        for start in phase_starts:
            plt.axvline(x=start, color='grey', linestyle='--')

        plt.title("Total Rewards Over Episodes")
        plt.xlabel("Episode")
        plt.ylabel("Total Reward (Moving Average)")
        plt.legend()
        plt.show()

    def plot_epsilon(self):
        """Plot the epsilon values over episodes for debug purposes."""
        plt.figure(figsize=(12, 6))
        for team_id, epsilons in self.epsilon_history.items():
            plt.plot(epsilons, label=f"Team {team_id} Epsilon")
        plt.title("Epsilon Decay Over Episodes")
        plt.xlabel("Episode")
        plt.ylabel("Epsilon")
        plt.legend()
        plt.show()


# Debug draft environment
player_data = pd.DataFrame({
    "player_name": ["QB1", "QB2", "QB3", "QB4", "QB5", "RB1", "RB2", "RB3", "RB4", "RB5",
                    "WR1", "WR2", "WR3", "WR4", "WR5", "TE1", "TE2", "TE3", "TE4", "TE5"],
    "position": ["QB", "QB", "QB", "QB", "QB", "RB", "RB", "RB", "RB", "RB",
                 "WR", "WR", "WR", "WR", "WR", "TE", "TE", "TE", "TE", "TE"],
    "projected_points": [360, 330, 300, 270, 240, 280, 220, 180, 150, 120,
                         210, 170, 150, 140, 120, 140, 110, 80, 70, 60]
})

# Pandas database of 400 player draft board from FantasyPros.com
# player_data = pd.read_csv("../Best_Ball/Best_Ball_Draft_Board.csv").drop('Unnamed: 0', axis=1).rename(columns={
#     "Player": "player_name", "POS": "position", "Fantasy Points": "projected_points"})

# Setup draft environment.
num_teams = 1
num_rounds = 4
position_limits = {"QB": 1, "RB": 1, "WR": 1, "TE": 1}
draft_simulator = FantasyDraft(player_data, num_teams, num_rounds)

# Setup training routine.
epsilons = [1.0, 0.5, 0.3, 0.2]
epsilon_mins = [0.5, 0.1, 0.05, 0]
epsilon_decays = [0.99965, 0.9985, 0.9975, 0.99]
num_episodes = [2000, 1500, 1000, 500]

# Run agents through an incremented training routine.
for phase in range(len(num_episodes)):
    for agent in draft_simulator.agents:
        agent.epsilon = epsilons[phase]
        agent.epsilon_min = epsilon_mins[phase]
        agent.epsilon_decay = epsilon_decays[phase]

    print(f"Beginning training phase {phase + 1}...")
    draft_simulator.train(num_episodes=num_episodes[phase], verbose=False)
    print(f"Training phase {phase + 1} complete!\n")

# Plot rewards and epsilons for debug purposes.
draft_simulator.plot_rewards()
draft_simulator.plot_epsilon()

# Run a singe episode after training with no exploration to get final results.
for agent in draft_simulator.agents:
    agent.epsilon, agent.epsilon_decay, agent.epsilon_min = 0, 0, 0
draft_simulator.run_episode(verbose=True)
