"""
This file constructs and trains a multi-agent Q-Learning algorithm with epsilon-greedy exploration to optimally draft
a fantasy football team.
We take as input the Best_Ball_Draft_Board.cvs generated by Best_Ball_Draft_Board.py
This is very much a work in progress with this being the first practice step in implementing increasingly complex
algorithms.
"""

import pandas as pd
import random
import matplotlib.pyplot as plt
from collections import defaultdict


class QAgent:
    def __init__(self, team_id, learning_rate=0.2, discount_factor=0.9, epsilon=1.0, epsilon_decay=0.995,
                 epsilon_min=0.01):
        """ Initialize an individual agent for a team. """
        self.team_id = team_id  # Team identification for this agent
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.q_table = defaultdict(float)  # Q-table to store state-action values for this agent
        self.drafted_players = []  # List to store drafted players for this agent
        self.total_reward = 0  # Store the total accumulated reward for this agent
        self.total_points = 0  # Store total projected fantasy points for this agent.
        self.position_counts = {"QB": 0, "RB": 0, "WR": 0, "TE": 0}  # Track drafted positions

    def reset_agent(self):
        """Reset the agent's state for a new episode."""
        self.drafted_players = []
        self.total_reward = 0
        self.total_points = 0
        self.position_counts = {"QB": 0, "RB": 0, "WR": 0, "TE": 0}  # Track drafted positions

    def get_state(self):
        """Get the current state representation for the agent."""
        return tuple(sorted(self.position_counts.values()))

    def choose_action(self, state):
        """Choose an action using an epsilon-greedy policy."""
        if random.random() < self.epsilon:
            return random.choice(list(self.position_counts.keys()))  # Random position
        else:
            return max(self.position_counts.keys(),
                       key=lambda position: self.q_table[(state, position)])  # Best position

    def update_q_table(self, state, action, reward, next_state):
        """Update the Q-table using the Q-learning formula."""
        best_next_action = max(self.position_counts.keys(), key=lambda position: self.q_table[(next_state, position)],
                               default=0)
        td_target = reward + self.discount_factor * self.q_table[(next_state, best_next_action)]
        td_delta = td_target - self.q_table[(state, action)]
        self.q_table[(state, action)] += self.learning_rate * td_delta


class FantasyDraft:
    def __init__(self, player_data, num_teams, num_rounds, position_limits):
        """ Initialize the multi-agent draft simulation. """
        self.player_data = player_data.sort_values(by="projected_points", ascending=False)  # Expects a pandas DataFrame.
        self.num_teams = num_teams
        self.num_rounds = num_rounds
        self.position_limits = position_limits
        self.agents = [QAgent(team_id=i) for i in range(num_teams)]  # Initialize an agent for each team.
        self.reward_history = {i: [] for i in range(num_teams)}  # Track rewards for debug purposes.
        self.epsilon_history = {i: [] for i in range(self.num_teams)}  # Track epsilon for each agent for debug purposes.
        self.draft_order = list(range(num_teams))
        self.reset_draft()

        self.max_points_by_position = player_data.groupby("position")["projected_points"].max()  # Cache max possible points by position for reward normalization.

    def reset_draft(self):
        """Reset the draft for a new episode."""
        self.available_players = self.player_data.copy()
        self.current_round = 0
        self.current_team = 0
        self.draft_order = list(range(self.num_teams))  # Reset draft order
        for agent in self.agents:
            agent.reset_agent()


    def run_episode(self, verbose=False):
        """Run a single episode of the draft."""
        self.reset_draft()
        while self.current_round < self.num_rounds:
            for team in self.draft_order:
                agent = self.agents[team]
                state = agent.get_state()

                # Agent chooses a position to draft
                position = agent.choose_action(state)

                # Get the top player for the chosen position
                available_position_players = self.available_players[self.available_players["position"] == position]

                # If there are no available players at the action position, punish and lose turn.
                if available_position_players.empty:
                    reward = -1
                    agent.total_reward += reward
                    next_state = state
                    agent.update_q_table(state, position, reward, next_state)
                    continue

                drafted_player = available_position_players.iloc[0]  # Assumes draft board is sorted by projected_points
                drafted_player_index = drafted_player.name

                # Update agent stats
                agent.total_points += drafted_player["projected_points"]
                agent.drafted_players.append(drafted_player["player_name"] + " " + drafted_player["Rank"])
                agent.position_counts[position] += 1

                reward = self.get_reward(drafted_player, agent)
                agent.total_reward += reward

                self.available_players = self.available_players.drop(drafted_player_index)

                next_state = agent.get_state()
                agent.update_q_table(state, position, reward, next_state)

            self.current_round += 1  # Move to next round after all teams have picked.
            self.draft_order.reverse()  # Reverse the draft order for snake draft formats.

        # Print episode summary
        if verbose:
            sum_rewards, sum_points = 0, 0
            for agent in self.agents:
                sum_rewards += agent.total_reward
                sum_points += agent.total_points
                print(
                    f"  Team {agent.team_id}: Total Reward = {round(agent.total_reward, 2)}, Drafted Players = {agent.drafted_players} ({round(agent.total_points, 2)} pts)")
            avg_reward = sum_rewards / num_teams
            avg_points = sum_points / num_teams
            print(f"Average total reward = {avg_reward}, Average total fantasy points = {avg_points}")


    def get_reward(self, drafted_player, agent):
        """Calculate the reward attained for drafting a given player by normalizing it with respect to the maximum
        possible points for that position. If we are exceeding position limits, give negative reward."""
        reward = drafted_player["projected_points"] / self.max_points_by_position[drafted_player["position"]]
        if agent.position_counts[drafted_player["position"]] > self.position_limits[drafted_player["position"]]:
            over_draft_penalty = agent.position_counts[drafted_player["position"]] - self.position_limits[
                drafted_player["position"]]  # Reward 2
            if 0 in agent.position_counts.values():
                over_draft_penalty += 1  # Reward 3
            reward = -(over_draft_penalty * reward)
        return reward

    def train(self, num_episodes, verbose=False):
        """Train the agents over multiple episodes."""
        for episode in range(num_episodes):
            self.run_episode(verbose=False)
            for agent in self.agents:
                agent.epsilon = max(agent.epsilon * agent.epsilon_decay, agent.epsilon_min)  # decay epsilon value.
                self.epsilon_history[agent.team_id].append(agent.epsilon)  # Log epsilon values
                self.reward_history[agent.team_id].append(agent.total_reward)  # Log rewards for debug purposes.

            # Print training status
            if verbose:
                print(f"Episode {episode + 1}/{num_episodes} completed.")

    def plot_rewards(self):
        """Plot the learning progress for debug purposes."""
        # Plot total rewards
        plt.figure(figsize=(12, 6))
        for team_id, rewards in self.reward_history.items():
            # Compute a moving average for total rewards.
            smoothed_rewards = pd.Series(rewards).rolling(window=50).mean()
            plt.plot(smoothed_rewards, label=f"Team {team_id + 1} Total Rewards")

        # Overlay vertical lines to represent the start of each phase
        phase_starts = [sum(num_episodes[:i]) for i in range(1, len(num_episodes))]
        for start in phase_starts:
            plt.axvline(x=start, color='grey', linestyle='--')

        plt.title("Total Rewards Over Episodes")
        plt.xlabel("Episode")
        plt.ylabel("Total Reward (Moving Average)")
        plt.legend()
        plt.show()

    def plot_epsilon(self):
        """Plot the epsilon values over episodes for debug purposes."""
        plt.figure(figsize=(12, 6))
        for team_id, epsilons in self.epsilon_history.items():
            plt.plot(epsilons, label=f"Team {team_id} Epsilon")
        plt.title("Epsilon Decay Over Episodes")
        plt.xlabel("Episode")
        plt.ylabel("Epsilon")
        plt.legend()
        plt.show()


# Debug draft environment
# player_data = pd.DataFrame({
#     "player_name": ["QB1", "QB2", "QB3", "QB4", "QB5", "RB1", "RB2", "RB3", "RB4", "RB5",
#                     "WR1", "WR2", "WR3", "WR4", "WR5", "TE1", "TE2", "TE3", "TE4", "TE5"],
#     "position": ["QB", "QB", "QB", "QB", "QB", "RB", "RB", "RB", "RB", "RB",
#                  "WR", "WR", "WR", "WR", "WR", "TE", "TE", "TE", "TE", "TE"],
#     "projected_points": [360, 330, 300, 270, 240, 280, 220, 180, 150, 120,
#                          210, 170, 150, 140, 120, 140, 110, 80, 70, 60]
# })

# Pandas database of 400 player draft board from FantasyPros.com
player_data = pd.read_csv("../Best_Ball/Best_Ball_Draft_Board.csv").drop('Unnamed: 0', axis=1).rename(columns={
    "Player": "player_name", "POS": "position", "Fantasy Points": "projected_points"})

# Setup draft environment.
num_teams = 12
num_rounds = 20
position_limits = {"QB": 3, "RB": 7, "WR": 8, "TE": 3}
draft_simulator = FantasyDraft(player_data, num_teams, num_rounds, position_limits)

# Setup training routine.
epsilons = [1.0, 0.5, 0.25]
epsilon_mins = [0.1, 0.05, 0]
epsilon_decays = [0.9993, 0.9985, 0.99]
num_episodes = [3000, 1500, 500]

# Run agents through an incremented training routine.
for phase in range(len(num_episodes)):
    for agent in draft_simulator.agents:
        agent.epsilon = epsilons[phase]
        agent.epsilon_min = epsilon_mins[phase]
        agent.epsilon_decay = epsilon_decays[phase]

    print(f"\nBeginning training phase {phase + 1}. Number of episodes in this phase is {num_episodes[phase]}.")
    draft_simulator.train(num_episodes=num_episodes[phase], verbose=False)
    print(f"Training phase {phase + 1} complete. Running a test draft with no exploitation.")
    for agent in draft_simulator.agents:
        agent.epsilon, agent.epsilon_decay, agent.epsilon_min = 0, 0, 0
    draft_simulator.run_episode(verbose=True)

# Plot rewards and epsilons for debug purposes.
draft_simulator.plot_rewards()
# draft_simulator.plot_epsilon()
